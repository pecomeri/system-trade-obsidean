ご提示いただいたファイル群（ディレクトリ構造、Markdown定義書、レビュー用コンテキスト）を拝見しました。

Webエンジニアとしての**「関心事の分離（Separation of Concerns）」**がディレクトリ構造（Setup / Entry / Filter / Strategy）に見事に反映されており、非常に拡張性の高い設計です 1。

しかし、**「スヌーピング（データへの過剰適合）」**という観点では、現在の`review_context.md`のTODO状態と結果フォルダの増え方を見ると、**「結果を見てから物語を作ってしまう」リスク**が高い状態にあります。

以下に (1) 破綻点、(2) 改善提案、(3) 具体的な次アクション をまとめます。

---

### 1. 破綻点（Snooping Risk & Bottlenecks）

検証が「堂々巡り」になり、本番で通用しないモデルを作ってしまうリスクが3点あります。

#### ① 「事後選択」によるバイアス (Selection Bias)

tree_fx.txt の results フォルダには b001〜b005、c001〜c005 と多数のバリエーション（Ablation Test：機能抜き取りテスト）の跡があります 2222。

しかし、review_context.md の「評価してほしい観点」が TODO のままです 3。

- **リスク:** 複数のバックテスト結果を並べて、「一番右肩上がりのもの（例えば `b002`）」を採用したくなります。これは**「たまたまその期間に合ったパラメータを選んだだけ」**になりやすく、典型的なスヌーピングです。
    

#### ② 定義の「揺らぎ」 (Ambiguity in Logic)

`S-001`（セットアップ）や `E-B001`（エントリー）の中身が、項目はあるものの具体的なロジック（Yes/No定義）が空欄、または「実装メモ」レベルです 4444。

- **リスク:** コード実装時（`core_setup_s001.py` 等）に、バックテスト結果を見ながら「この判定は厳しすぎるから緩めよう」と**アドホックな調整**が入る余地があります。仕様と実装が乖離し、再現性が失われます。
    

#### ③ 検証ループの「出口」が未定義 (Undefined Exit Criteria)

`review_context.md` に「いま一番迷っている論点」が `TODO` となっています 5。

- **リスク:** 「何を確認したら、このSetup S-001は採用/不採用とするか」が決まっていないため、少しでも悪い結果が出ると「微調整（Curve Fitting）」を繰り返す無限ループに陥ります。
    

---

### 2. 改善提案（Design for "Snooping-Proof" Workflow）

エンジニアリング力を活かし、**「仮説検証プロセス自体をコード化（プロトコル化）」**します。

#### A. 事前コミットメント（Pre-commitment）の徹底

review_context.md を「結果を見るためのメモ」ではなく、**「実行前の契約書」**にします。

結果を見る前に以下を埋めてください。

> - **検証仮説:** 「H1フィルター（上位足順張り）を外すと、取引数は増えるがPFは0.1以上悪化するはずだ」
>     
> - **判定基準:** 「2024年のデータでPFが低下しなかった場合、H1フィルターは不要（無駄な複雑性）と判断して削除する」
>     

#### B. In-Sample / Out-of-Sample の厳格な分離

results フォルダですでに in_sample_2024 と forward_2025 が分かれているのは素晴らしい設計です 6。

運用ルール:

- **開発・改善:** すべて `in_sample_2024` だけで行う。
    
- **封印解除:** ロジックが完全に固まった最後に一度だけ `forward_2025` を見る。
    
- **禁止事項:** 2025年の結果を見てからロジックを修正してはならない（これを行うと、2025年にも過学習します）。
    

#### C. Setup/Entryの「仕様凍結」

Markdownファイル（`S-001`, `E-B001`）をドラフトではなく、Go言語やPythonコードに落とせるレベルの**擬似コード**で埋めます。

- 例: `トレンドの定義` → 「SMA20 > SMA50 かつ 直近高値 > 前回高値」のように、曖昧さを排除する。
    

---

### 3. 具体的な次アクション（Next 3 Steps）

この週末などで、以下の順序で手を動かすことを推奨します。

#### Step 1: 仕様の「Yes/No」を埋める

まず、コードを書く・回すのを止め、ロジック定義を確定させます。

- **Action:** `S-001_trend_midrange_fakeout_v.md` と `E-B001_rebreak_confirm.md` の「Yes/No 定義」セクションに、**これ以上解釈の余地がない条件**を記述してください。
    
    - _（もしここで迷うなら、単純な定義を選んでください。複雑さは敵です）_
        

#### Step 2: 「判定基準」を `review_context.md` に書く

次に、今回の検証サイクルで何を「勝ち」とするかを定義します。

- **Action:** `review_context.md` の「目的」と「評価してほしい観点」を埋めてください。
    
    - 記述例：「B002（H1フィルターあり）とB003（なし）を比較し、B002の方がPFが高い、かつドローダウンが低い場合のみ、B002の構成を採用して次へ進む。」
        

#### Step 3: Verify（2024年のみ）を実行し、結論を出す

最後に、一度だけ実行します。

- **Action:** Step 1の定義に基づき、2024年データ（In-Sample）のみでバックテストを実行。Step 2の基準に照らし合わせて**「このロジックを採用する」か「捨てる」かを即断**し、その結果をMarkdownに記録してクローズしてください。2025年のデータはまだ見ないでください。
    

---

この3ステップを踏むことで、「なんとなく良さそうな設定を探す旅」から、「仮説を科学的に棄却/採用するエンジニアリング」へと切り替わります。

**まずは `S-001` の「定義」セクションを埋めるお手伝いをしましょうか？** それとも `review_context.md` の判定基準の策定を行いますか？